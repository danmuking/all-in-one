先抛开mallchat的实现不谈，我们大胆假设下，想要实现一个**完善**的IM系统，需要考虑哪些方面？
![im常见问题.png](https://raw.githubusercontent.com/danmuking/image/main/ba245c0c12c421416113eca076049a38.png)
我考虑了常见的几个关键点，整理了以下这张消息的架构图。接下来的文章，会是整个mallchat最重要的部分。
一个千万级的消息系统，任何一个小功能，都可能成为性能瓶颈，都需要复杂的设计去支撑。我会和你一起去探讨每一个功能，应该考虑哪些方面，每种方案的优劣，以及mallchat是如何实现的。
> 由于mallchat是web项目，不会存储消息，和客户端IM不同，因此，mallchat的一些实现方案并不是最好的，但我们会聊聊怎样设计能更好，这样在被问到的时候，也能有底气的回答。

![集群推送 (1).jpg](https://raw.githubusercontent.com/danmuking/image/main/a7d52496f40e939389687eee555d3c44.jpeg)
`WebSocket`：维护和用户的连接通道，可以接收消息，也可以推送消息，为**有状态服务**
`IM服务`：负责消息的发送逻辑，处理单聊群聊的消息
`Logic服务`：处理用户的心跳，上下线，联系人，加好友，创群组等逻辑
`Auth服务`：处理用户认证，权限等需求
`Router`：推送消息时，不同用户在不同`WebSocket`服务上，确保正确推送，与可靠推送。
交互的流程大致如下：

1. 用户A和`WebSocket`服务建立连接。之后都通过该连接发送消息，接受消息。
2. 用户A发送了一条群消息“在吗”，`WebSocket`服务将消息通过dubbo转发给`IM服务`，由于`IM服务`是无状态的，可以通过负载均衡随机发到某一台上。
3. `IM服务`将消息持久化，然后将消息投递到`消息队列MQ`，这样能快速响应前端，并且mq的消费者根据负载慢慢的进行后续的推送，写扩散等操作。
4. `消费者`会判断，根据是否热点群聊的消息做不同逻辑。如果是热点群聊，只写`热点信箱`。如果是单聊或者普通群聊，会[写扩散](https://www.yuque.com/danmu-ix6zd/vaqzuz/eu9gppkq2igmv3gx)到每个`群成员信箱`。这里假设是小群，会写入B和C的信箱。
5. 将消息投递信箱后，需要将消息推送给用户。这里可以根据是否在线，在线的进行`WebSocket`推送，离线的进行`push通知`。由于用户的连接在不同的`WebSocket`上，需要`Router`服务推送到B和C所在的不同`WebSocket`方案有两种，后续介绍。
6. 推送的时候需要确保消息的可靠性，如果保证一定推送成功？可能要做`应用层的ack`，类似tcp的滑动窗口确认。
7. 用户在查询自己的会话列表的时候，需要有一个`聚合层`聚合`用户信箱`，以及`热点信箱`。再严格排序后返回给用户。所谓之`推拉结合`。
## 集群推送
http和websocket的底层其实都是依赖tcp建立连接进行通信的。他们的差别就是：

1. http是无状态的。每次请求都会重新握手建立tcp链接再发送消息，并获取响应。因此http请求可以随意的进行负载均衡。第一次请求发给了机器A，第二次请求就发给了机器B。
2. websocket是有状态的，当他建立tcp连接后，就会一直复用那个连接进行通信。假设一开始连上了机器A，收发信息就一直在机器A。直到连接断开，重连后才会更换连接。

![](https://raw.githubusercontent.com/danmuking/image/main/3249fc604acc34298bf6b81fce49415d.jpeg)
我们项目用的[netty](https://www.yuque.com/snab/mallchat/skb0r8tesr7yitvf)来实现websocket，和用户的连接就是一个channel。需要给用户推送消息的时候，直接往`channel`里面`write`消息就好了。
```java
/**
 * 给本地channel发送消息
 *
 * @param channel
 * @param wsBaseResp
 */
private void sendMsg(Channel channel, WSBaseResp<?> wsBaseResp) {
    channel.writeAndFlush(new TextWebSocketFrame(JSONUtil.toJsonStr(wsBaseResp)));
}
```
在之前的[微信登录技术方案](https://www.yuque.com/snab/mallchat/bsld797perd3xb85)，我们已经描述了如何在用户登录的时候，将uid和channel关联起来缓存在jvm的map里，这样要推送的时候只需要通过uid取出channel进行推送。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/4af8d9eedd0c6e8903ea58db4de3e0b4.png)
但是在集群的场景，这样的方案就失效了。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/b34cc14d25656f4db95565f447f9cc90.png)
你要推送的用户，在别的机器上，你怎么找到对应的机器？根本的原因是因为连接的管理在jvm层面，假设A要发消息给C，我们不知道C在哪一台`WebSocket`。所以这时候可以借助一个中心化的中间件，比如redis，来存储他们的关系。
### redis存储Channel
这个方案是最不可行的方案，也是最搞笑的方案。由于channel和uid的关系存在了**本地**，导致我们想推送消息给C的时候拿不到channel。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/dea8963cff4ec9a9d4039fa00ccc2b98.png)
有个小伙伴想着既然`websocket`集群部署了，那就用中心化的redis来存`uid`和`channel`映射关系。通过`redis.get(uid)`拿到字符串，并反序列化成`channel`，然后发送消息。
```java
private void sendMsg( WSBaseResp<?> wsBaseResp,Long uid) {
    Channel channel = RedisUtils.get(uid.toString(), Channel.class);
    channel.writeAndFlush(new TextWebSocketFrame(JSONUtil.toJsonStr(wsBaseResp)));
}
```
其实这样会报错的，channel是本地的socket连接，没法进行存储与反序列化。
### 精准投递消息
可以对上面的方案优化下，本地依然要维护uid和channel的关系（这可是根基），redis再维护一个用户的连接状态，比如用户在哪台机器上连接。这样通过router发送消息的时候，就知道用户的消息应该发送到哪台机器上。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/dd4b1a886678d71b8b4c6d4bc932b7be.png)
大概的流程如下：

1. A发送了一条给C的消息。通过`channel`发送给了`10.102.1.1`这台`websocket`，它通过dubbo将消息随意转发给了一台`IM服务`。
2. `IM服务`将消息持久化后，调用`Router`，推送消息给C
3. `Router`查redis的中心化管理，查到C目前连接在`10.102.1.2`上，并通过`tcp连接`将C的消息推送给`10.102.1.2`。Router会和所有`websocket`都维护一条tcp连接。查到具体的ip后，对指定`websocket`服务器进行消息推送。
4. `websocket`服务收到的请求格式为给`uid`发送`xx消息`，所以它会通过uid在本地的`连接管理`中，查出用户具体的`channel`。然后调用`channel.write（消息）`方法，给用户推送消息。

以上就是大致的流程，咱们只管集群的推送，不管推送的可靠性保证等问题，后续再讨论。
这个方案有啥问题？

1. 需要频繁的更新redis去维护用户和websocket服务的映射。（这个是小问题，因为我们正好需要做用户的上下线，也复用这个功能）
2. `**连接数爆炸**`：websocket的瓶颈就在连接数，如果连接满了，就要水平扩容websocket，这样才能支撑更多人同时在线。如果用户体量非常大，甚至需要上千个websocket和上千个router。

![image.png](https://raw.githubusercontent.com/danmuking/image/main/c0abcc65a99bb01b234218c44737900b.png)
这其实是一种很可怕的事，单纯的路由连接就达到上千，占用了websocket的连接资源，导致一台`websocket`能连接的用户数变少。
这种问题其实也会出现在dubbo服务中，dubbo单实例集群达到1000以上，他们是怎么做的呢？
由于传统的service服务都是**无状态**的，我们的连接可以**分组管理**。每台service只需要和少数的下游实例维护tcp连接即可，不需要连接所有。
但是websocket是有状态的，一台`router`必须连接所有的`websocket`，他的消息有可能需要路由到任意一台`websocket`上。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/8f7aabdbf0900d375f28ce81da6a28b0.png)
我们可以用分层路由的思想，中间加一层路由，设定路由规则。这样可以**有效减少连接数**，但是会**增加消息的推送链路**，适用于真的很大型的集群场景。
当然还有很多方案，甚至我们可以思考，为啥去保证tcp连接？如果每次发消息都建立tcp连接，那就是三次握手3倍的RTT。如果我们不在乎这些连接耗时，也可以直接采用**http推送**，或者**临时建立连接**的方式。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/c1d7119a7d7f21ca57029b13b8829e39.png)
这种订阅表，路由节点，就也有点像分布式集群的mq，感兴趣可以看看[文档](https://www.emqx.io/docs/zh/v5.2/deploy/cluster/introduction.html)

1. 实现复杂，需要指定ip推送，维护tcp连接。如果用dubbo会稍微简单些。
2. `**消息发送开销**`：对于群聊，多个接收者需要发送多份消息的副本，增加了消息发送的开销（这个对比后面的方案就能理解）
3. `**延迟叠加**`**：** 哪怕开启了多线程，也依然会有一台机器的单点，需要对所有的群成员进行消息的**扇出**（写扩散）。这里会导致接收者接到的消息延迟叠加。（当然，用线程池异步扇出可以提高速度，但是会回到问题4，总体开销依然是不变的）
4. `**雪崩问题**`：如果系统负载提高，比如需要推送的消息量突然变大，导致瓶颈从`websocket`一直传导到`router`再传到`im服务`，整个集群会出现雪崩，需要有一个消息队列来进行削峰填谷。
### 集群广播消息
针对于以上精准推送的问题，如果我告诉你，有一种方案，能够**无需维护连接**，消息的发送只需要一个**消息副本**，并且没有**消息扇出**的压力，只需要写一次。你满不满意？它就是集群广播。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/2c4a7a26f657f064464631d5ae8f78e2.png)
对于万人群聊，一般系统的压力就在于消息的**扇出**（**写扩散**）。如果按照精准投递的话，我们的消息需要查询redis中心路由，然后将消息投递1w次。而如果用消息广播的形式，消息只需要投递一次。由websocket自己进行广播消息的**拉取**与**过滤。**
#### 过滤流程
![image.png](https://raw.githubusercontent.com/danmuking/image/main/4b76e1cb5c0b81662328f9f11f18c30b.png)

1. mq的消息消费模式为集群消费，确保每台`websocket`都能消费到所有需要投递的消息。
2. 对比推送的uid在不在`本地连接管理`的列表，如果不在，直接丢弃消息，也叫过滤消息。
3. 如果在本地连接管理，根据`uid`取出`channel`，就可以进行消息推送了。
#### 优化1：消息副本优化
集群广播和精准投递比起来，消息副本少了很多。但是如果用刚刚的图展示，反而看不出来效果。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/b8f0b5c57a05b880c771bc9e93168c81.png)
这样比较起来，反而是集群广播的消息副本整体多了。假设B发送消息给**A**和**C**。
精准投递总共传输了2*3=6份消息，集群广播传输了2*4=8份消息。
既然是集群广播，我们就应该利用好广播的特性。
一份消息由uid和消息体组成，uid就是对应的**紫色方块**，消息体就是对应的**黑色信封**，一般消息体远大于uid。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/766ea94bdd1d43cd52daaf1a6908a1aa.png)
在万人群聊下，其实大家收到的消息都一样的，可以共用一个消息副本，我们在投递消息进mq的时候，可以设置投递的消息为`list<long>`的`uids`。而不是拆成多次的消息投递.
改造后的效果如下：
![image.png](https://raw.githubusercontent.com/danmuking/image/main/6caf21e88f53f8acce41ca203e148b54.png)
这差别可就大了，假设是万人群聊。
精准投递总共传输了**1w*3=3w**份消息， **1w*3=3w**个uid，
集群广播传输了**1w+3**份消息，**2w+1w*2=4w**个uid。
因为终端投递是省不了的，但是可见消息副本数大大下降。大家可能会觉得有些赖皮，1w个连接涉及到的websocket其实会有更多个。消息大小远大于uid，我们以消息为准
```
精准投递：u*3份消息，u=群聊在线人数，3=消息传输的链路，固定值
集群广播：u+n份消息，u=群聊在线人数，n=websocket的集群数，不固定，但是极小。
```
可见该优化在大群聊下的效果有多强，当然其实集群广播方案也有缺点，就是在im中，**小群聊甚至单聊的占比**很大，在u很小，且n很大的情况下，集群广播就失去了他的优势。
#### 优化2：消息过滤优化
根据刚刚的过滤流程，我们可知道消息内的uid过滤，都是在本地消费者逻辑去过滤的，不存在就直接丢弃消息。这时候大量与本`websocket`无关的消息都被拉取过来了，中间的网络io都浪费了。这还真没啥办法解决。但是我们可以在海绵里挤水呀，想想还有啥能优化的。
> 大胆假设，小心求证

集群广播最大的毛病就在于，很多不属于我们websocket的消息推送，也会被我们读取，然后在本地过滤扔掉。这里浪费了**网络IO**，浪费了本地**反序列化的cpu**。
这两点能不能改善呢？？？
解决1：我们可以通过设置header的方式。将推送的uids存在header里。这样我们拉取消息到消费端的时候。就可以在序列化前，先在**mq的过滤器**里去过滤消息，**节省不必要的反序列化**。
这里需要注意一点，header有长度限制。我们需要注意大群聊uid过多分批发送。或者群聊压根不过滤，就考虑单聊场景。有效
解决2：上面的方案是在消费端才过滤，只节省了反序列化。我还想贪心点，在broker端过滤消息，直接节省IO的传输成本。
调研了两种broker端过滤方案，tag的方式首先不支持我们的需求，没法存到用户纬度那么大。我们可以用sql29的方式写表达式匹配。把uids写到消息的header里，sql29表达式可以去匹配。过滤出包含我们有的uid的消息。
但是sql29并不支持动态过滤。而我们websocket的连接用户是会一直变化的。每一次请求都需要做到不一样。
所以解决方案2的方案相当于破产了，但是rocketmq最大的**优势**，就是纯java编写。我们可以去**魔改源码**改broker的逻辑呀，专门改一个更适合im场景的mq过滤交互框架，也是一个不错的选择。无效

虽然方案不行，但是我依然带着大家去大胆假设，小心求证。目的是带着大家了解我们使用的工具，以及它的能力。**这是一次对rocketmq高级用法的探索**，和我们业务的完美结合，希望大家用心去思考，去感受。

对rocketmq的深入理解看《[阿斌Java之路](https://www.yuque.com/snab/java/vxtx3s#F9aaN)》，sql92的过滤原理，请[查看相关文章](https://blog.csdn.net/jjhfen00/article/details/132176491)，比较复杂，投递端就过滤了。

这样在被问起，我们能说出这样思考的过程，求证的过程，以及联动到rocketmq的面试题，就形成了一套自己的组合拳。在每次的学习中，不断的缔造自己错综复杂的知识网络，岂不美哉。
**这个方案有啥问题？**
其实问题也比较明显。每个websocket都需要接收**全站所有消息**的广播消息，然后内部进行过滤。如果全站的消息都是单聊消息，结果每个websocket都拉取消息，并且内部过滤，是带宽的浪费，以及cpu的浪费。
### 总结
根据前面的计算公式对比，我们很容易的得出一个简单的结论。
> 单聊消息多的，用精准投递。群聊消息多的，用广播消息。

对于抹茶，我们是有个全员群聊，很多个小群聊，和很多的单聊。发言频率最多的场景是全员群。所以抹茶最应该使用集群广播推送的方案。
接下来再讨论一个极端场景的解决方案。
#### 百万直播间推送方案
假设在抖音直播的场景下，一个热门的直播间**100w**人同时在线，大量的礼物，互动消息充斥在直播间，如何通知到每个人。保证消息的**即时性**，**可靠性**。
首先这明显更倾向于大群聊的一种场景，如果用精准投递，那么消息的扩散系数就是100w级。如果采用的是集群推送，假设100w的用户需要500台websocket进行连接，那么扩散系数只是500的级别。
但是这个假设是整个平台只有这个直播间，如果平台有更多的直播间。websocket会更多，mq的扩散系数也会更大。
![image.png](https://raw.githubusercontent.com/danmuking/image/main/aab2f3556de9a642f22ffe05f3501ca2.png)
每个方案又都优缺点，而应对极端场景，通常都是方案的组合，扬长避短。很类似于我们后面会提到的推拉结合。
这个场景的方案，我们可以设置一个热门阈值，比如1w。超过1w的直播间，我们会进行直播间升级，升级成热门直播间。热门直播间的websocket单独管理。把直播间用户的websocket连接都统一路由到固定的几百台websocket上。由于目标用户都集中了，也就不需要精准投递了，可以采用广播投递消息到这指定500台机器上。再对应的推送给直播间的观众。
**这里其实是精准投递和集群推送的一个结合（你会发现，很多方案都是有优劣的，最后都是结合起来使用扬长避短）**
这个方案的核心，就是要能将直播间所有用户通过**网关路由**到相同的500台websocket上，有了这个基础，才能用广播消息，那500台websocket都监听同一个topic的**广播mq消息**。能省下很大的带宽开销。而消息的发送端，需要知道消息究竟是发送到热门直播间的topic进行**集群广播**还是普通直播间的**精准推送**。还是得依赖**router**服务进行路由推送。

再讨论下面几个功能的细节：
`热门直播间升级`：一开始的普通直播间，用户都分散在不同的websocket机器上。等到直播间人数突破阈值1w。就需要开始直播间的**热点升级**。这时候服务器检测到直播间需要升级，**动态扩缩容**，启动一系列配套措施（k8s现在已经使用的比较多了）。一系列措施准备好后，相应的配置推送到网关路由机器上。指定以后该直播间的连接路由到我们新启动的50台websocket上。然后对当前在线的所有用户发送断连替换指令。所有在线用户都断开连接，重连的时候会被网关路由到新的websocket上。优化：对于经常突破1w人的直播间，可以打个标。以后该直播间上线，默认就是热点，省略升级过程。
`消息合并`：直播间的点赞操作，一般发生在主播求赞的时候，大量人在同一时间段点赞。并且单人在同一时间也快速点赞。可以在客户端对每个人的多次点赞首先进行合并一次（用户a点赞20）。请求到后端后，由于路由已经做好，在每个点赞服务器，可以对多人的点赞再合并一次（用户a+b总点赞40），进行入库。给前端推送的时候，也可以合并推送，不需要每条点赞都推送。每隔1s推送一次直播间点赞总量达到（100w）。
不了解请求合并思想的，可查看《架构之路》的文章[请求折叠工具类](https://www.yuque.com/snab/architecture/mqtmi1d5i07tm0w1)。
`优先级隔离`：在100w直播间里，推送的消息会有很多。会导致部分消息到达产生延迟。这就类似push系统，消息应该**区分优先级**，不要被互相影响。大礼物，和主播发言消息，这些应该独立在一个广播topic里，其他的不重要的消息，可以设置另一个topic，区分优先级，不要影响重要消息。
> 连这个极端场景都能搞定，其中的思想只要能理解透，还有啥im能够难倒你？

